{
  "paper_id": "apep_0088_v7",
  "scan_date": "2026-02-10T12:41:33.319175+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 14,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        35,
        36
      ],
      "evidence": "Key headline estimates and p-values are manually written into the manuscript rather than being auto-generated from model objects/tables produced by the code. This is not proof of manipulation, but it is an integrity risk because it enables transcription errors or selective updating (code changes without corresponding manuscript updates). Consider replacing these literals with \\input{} of LaTeX tables/values generated directly by the analysis scripts.: The primary specification---restricted to same-language borders to eliminate the French-German confound---yields an estimate of $-5.9$ percentage points ($p = 0.011$; wild cluster bootstrap $p \\approx 0.06$). A Difference-in-Discontinuities design controlling for permanent border effects confirms the negative direction ($-4.7$ pp, $p = 0.008$).",
      "confidence": 0.72
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        141,
        142,
        143,
        144,
        145,
        146
      ],
      "evidence": "If the fetch for 2000\u20132009 placebo referendums fails, the script silently writes an empty placeholder dataset that can allow downstream scripts to run without placebo diagnostics. This weakens provenance/traceability of placebo tests: later code may report 'placebo not available' rather than failing loudly. For research integrity, it is preferable to stop() (or at least set a hard failure flag) when placebo data are required for reported diagnostics.: placebo_votes_raw <- tryCatch({\n  ...\n}, error = function(e) {\n  message(paste(\"   Error fetching placebo votes:\", e$message))\n  NULL\n})\n\n# Save placebo votes if successfully fetched\nif (!is.null(placebo_votes_raw) && nrow(placebo_votes_raw) > 0) {\n  saveRDS(placebo_votes_raw, file.path(data_dir, \"placebo_votes_raw.rds\"))\n  ...\n} else {\n  # Create empty placeholder to prevent downstream errors\n  placebo_votes_raw <- tibble()\n  saveRDS(placebo_votes_raw, file.path(data_dir, \"placebo_votes_raw.rds\"))\n  message(\"   Created empty placebo_votes_raw.rds (fetch failed)\")\n}",
      "confidence": 0.78
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "11_didisc_analysis.R",
      "lines": [
        266,
        267,
        268,
        269,
        270,
        271
      ],
      "evidence": "The manuscript describes the DiDisc specification as including municipality fixed effects (\\alpha_i) and referendum fixed effects (\\delta_t), with an alternative adding border-pair fixed effects. In this code, the 'Border-Pair FE' model drops municipality fixed effects entirely and uses only border_pair + vote_type fixed effects. That changes the identifying variation and may not match the stated Eq. (didisc) when the paper claims municipality FE are included. If the intended model is \\alpha_i + \\delta_t + border_pair FE, it should be explicitly implemented (e.g., absorbing mun_id as well, or clarifying why border-pair FE replaces mun_id FE).: didisc_borderpair <- tryCatch({\n  feols(\n    yes_share ~ treated_post | border_pair + vote_type,\n    data = panel_bp,\n    cluster = ~canton_abbr\n  )\n}, error = function(e) {\n  ...\n})",
      "confidence": 0.74
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "04_robustness.R",
      "lines": [
        53,
        54,
        55,
        56,
        57
      ],
      "evidence": "Placebo referendums are selected via keyword matching on the referendum name. This is a reasonable heuristic, but it can be subjective and may omit relevant pre-treatment votes or include borderline cases. Because placebo tests are used as validity diagnostics, a pre-specified list of vote IDs/dates (or a documented selection protocol) would be more transparent.: placebo_energy <- placebo_votes %>%\n  filter(\n    votedate < \"2010-01-01\",  # Pre-treatment\n    str_detect(tolower(name), \"energie|energy|umwelt|environment|atom|nuclear|klima|co2\")\n  )",
      "confidence": 0.67
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        63,
        64,
        65
      ],
      "evidence": "The robustness script explicitly limits placebo RDDs to the first 5 matching pre-treatment votes. If there are more eligible placebo referendums, this creates a risk of selective reporting depending on ordering (which may be arbitrary or dependent on API return order). For integrity, run all eligible placebo votes or implement a deterministic, documented selection rule (e.g., 'the 5 most similar topics' with explicit IDs).: for (i in 1:min(nrow(unique_votes), 5)) {  # Limit to 5 placebos\n  vote_id <- unique_votes$id[i]\n  ...\n}",
      "confidence": 0.81
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        43
      ],
      "evidence": "A global random seed is set for reproducibility. This is typically benign, but it becomes relevant if any later steps use randomization (e.g., permutation inference, bootstrap, random train/test splits). Ensure any stochastic procedures are explicitly labeled and that reported results are not accidentally driven by a single seed without robustness checks. No direct simulated/fabricated data generation is visible in the provided files.: # Set seed for reproducibility\nset.seed(20260127)",
      "confidence": 0.6
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00b_verify_treatment.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "10_placebo_corrected.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "11_didisc_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_expanded_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "09_fix_rdd_sample.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_revision_fixes.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "12_corrected_rdd_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 4,
      "LOW": 2
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "/scans/apep_0088_v7_scan.json"
  },
  "error": null
}